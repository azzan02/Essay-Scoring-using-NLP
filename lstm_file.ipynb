{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bfuB6ewD0Aak",
        "outputId": "af46cd03-712b-4c82-9b2e-8c26a7627419"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   Unnamed: 0  essay_id  essay_set  \\\n",
            "0           0         1          1   \n",
            "1           1         2          1   \n",
            "2           2         3          1   \n",
            "3           3         4          1   \n",
            "4           4         5          1   \n",
            "\n",
            "                                               essay  final_score  \\\n",
            "0  Dear local newspaper, I think effects computer...            6   \n",
            "1  Dear I believe that using computers will benef...            7   \n",
            "2  Dear, More and more people use computers, but ...            5   \n",
            "3  Dear Local Newspaper, I have found that many e...            8   \n",
            "4  Dear I know having computers has a positive ef...            6   \n",
            "\n",
            "                                         clean_essay  char_count  word_count  \\\n",
            "0  Dear local newspaper  I think effects computer...        1441         344   \n",
            "1  Dear I believe using computers benefit us many...        1765         413   \n",
            "2  Dear  More people use computers  everyone agre...        1185         276   \n",
            "3  Dear Local Newspaper  I found many experts say...        2284         490   \n",
            "4  Dear I know computers positive effect people  ...        2023         469   \n",
            "\n",
            "   sent_count  avg_word_len  spell_err_count  noun_count  adj_count  \\\n",
            "0          16      4.188953               11          76         75   \n",
            "1          17      4.273608               21          98         84   \n",
            "2          14      4.293478                5          76         51   \n",
            "3          26      4.661224               31         142         96   \n",
            "4          30      4.313433               18         110         90   \n",
            "\n",
            "   verb_count  adv_count  \n",
            "0          18         24  \n",
            "1          20         19  \n",
            "2          20         16  \n",
            "3          39         29  \n",
            "4          32         36  \n"
          ]
        }
      ],
      "source": [
        "# prompt: open Processed_data.csv\\\n",
        "\n",
        "import pandas as pd\n",
        "\n",
        "try:\n",
        "  df = pd.read_csv('Processed_data.csv')\n",
        "  print(df.head()) # Print first few rows to verify\n",
        "except FileNotFoundError:\n",
        "  print(\"Error: 'Processed_data.csv' not found.\")\n",
        "except pd.errors.EmptyDataError:\n",
        "  print(\"Error: 'Processed_data.csv' is empty.\")\n",
        "except pd.errors.ParserError:\n",
        "  print(\"Error: Unable to parse 'Processed_data.csv'. Check the file format.\")\n",
        "except Exception as e:\n",
        "  print(f\"An unexpected error occurred: {e}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Embedding, LSTM, Dense, Dropout\n",
        "from sklearn.model_selection import train_test_split"
      ],
      "metadata": {
        "id": "XmlWlJwI02Iy"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "max_words = 10000  # Vocabulary size\n",
        "max_len = 200      # Maximum length of sequences\n",
        "embedding_dim = 128\n",
        "\n",
        "# Text Tokenization and Padding\n",
        "tokenizer = Tokenizer(num_words=max_words, oov_token=\"<OOV>\")\n",
        "tokenizer.fit_on_texts(df['essay'])\n",
        "sequences = tokenizer.texts_to_sequences(df['essay'])\n",
        "padded_sequences = pad_sequences(sequences, maxlen=max_len, padding='post', truncating='post')"
      ],
      "metadata": {
        "id": "DRr1bk-p1OiE"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "scores = df['final_score'].values\n",
        "\n",
        "# Train-test split\n",
        "X_train, X_val, y_train, y_val = train_test_split(padded_sequences, scores, test_size=0.2, random_state=42)\n"
      ],
      "metadata": {
        "id": "xN1lMuQR1Sdl"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_train = y_train / 10.0  # Assuming scores are out of 10\n",
        "y_val = y_val / 10.0\n"
      ],
      "metadata": {
        "id": "mOQP9_Vy1bCa"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = Sequential([\n",
        "    Embedding(input_dim=max_words, output_dim=embedding_dim, input_length=max_len),\n",
        "    LSTM(128, return_sequences=False),\n",
        "    Dropout(0.3),\n",
        "    Dense(64, activation='relu'),\n",
        "    Dropout(0.3),\n",
        "    Dense(1, activation='sigmoid')  # Use 'sigmoid' for normalized scores\n",
        "])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LxDHsoSR1fDY",
        "outputId": "8620938f-2f98-4737-a540-f77953331d5f"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/layers/core/embedding.py:90: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.compile(optimizer='adam', loss='mean_squared_error', metrics=['mae'])"
      ],
      "metadata": {
        "id": "5vRxGpNS1iT5"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "history = model.fit(\n",
        "    X_train, y_train,\n",
        "    validation_data=(X_val, y_val),\n",
        "    epochs=10,\n",
        "    batch_size=32,\n",
        "    verbose=1\n",
        ")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cPvr2Aj11mVn",
        "outputId": "8294f4d9-2cb9-4348-f264-b268f8417ad3"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "\u001b[1m325/325\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m136s\u001b[0m 408ms/step - loss: 0.0576 - mae: 0.1934 - val_loss: 0.0537 - val_mae: 0.1871\n",
            "Epoch 2/10\n",
            "\u001b[1m325/325\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m141s\u001b[0m 405ms/step - loss: 0.0463 - mae: 0.1718 - val_loss: 0.0341 - val_mae: 0.1458\n",
            "Epoch 3/10\n",
            "\u001b[1m325/325\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m139s\u001b[0m 394ms/step - loss: 0.0360 - mae: 0.1486 - val_loss: 0.0300 - val_mae: 0.1364\n",
            "Epoch 4/10\n",
            "\u001b[1m325/325\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m141s\u001b[0m 393ms/step - loss: 0.0333 - mae: 0.1433 - val_loss: 0.0255 - val_mae: 0.1235\n",
            "Epoch 5/10\n",
            "\u001b[1m325/325\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m142s\u001b[0m 394ms/step - loss: 0.0255 - mae: 0.1233 - val_loss: 0.0240 - val_mae: 0.1208\n",
            "Epoch 6/10\n",
            "\u001b[1m325/325\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m140s\u001b[0m 389ms/step - loss: 0.0222 - mae: 0.1155 - val_loss: 0.0241 - val_mae: 0.1207\n",
            "Epoch 7/10\n",
            "\u001b[1m325/325\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m127s\u001b[0m 392ms/step - loss: 0.0190 - mae: 0.1057 - val_loss: 0.0245 - val_mae: 0.1219\n",
            "Epoch 8/10\n",
            "\u001b[1m325/325\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m141s\u001b[0m 389ms/step - loss: 0.0155 - mae: 0.0953 - val_loss: 0.0251 - val_mae: 0.1216\n",
            "Epoch 9/10\n",
            "\u001b[1m325/325\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m129s\u001b[0m 396ms/step - loss: 0.0140 - mae: 0.0894 - val_loss: 0.0259 - val_mae: 0.1229\n",
            "Epoch 10/10\n",
            "\u001b[1m325/325\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m142s\u001b[0m 396ms/step - loss: 0.0115 - mae: 0.0814 - val_loss: 0.0271 - val_mae: 0.1257\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "loss, mae = model.evaluate(X_val, y_val, verbose=1)\n",
        "print(f\"Validation MAE: {mae}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nblH2ior1qpK",
        "outputId": "45bc7a4b-f57d-49ec-880a-d4d7fecf062b"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m82/82\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 127ms/step - loss: 0.0281 - mae: 0.1280\n",
            "Validation MAE: 0.12565644085407257\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def predict_score(essay, tokenizer, model, max_len=200, scale_factor=10.0):\n",
        "    \"\"\"\n",
        "    Predicts the final score for a given essay.\n",
        "\n",
        "    Parameters:\n",
        "    - essay (str): The essay text to score.\n",
        "    - tokenizer (Tokenizer): The trained tokenizer used during model training.\n",
        "    - model (Sequential): The trained LSTM model.\n",
        "    - max_len (int): Maximum length of sequences (used for padding). Default is 200.\n",
        "    - scale_factor (float): The factor to scale the normalized score. Default is 10.0.\n",
        "\n",
        "    Returns:\n",
        "    - float: The predicted final score.\n",
        "    \"\"\"\n",
        "    # Preprocess the input essay\n",
        "    sequence = tokenizer.texts_to_sequences([essay])  # Tokenize\n",
        "    padded_sequence = pad_sequences(sequence, maxlen=max_len, padding='post', truncating='post')\n",
        "\n",
        "    # Predict using the model\n",
        "    normalized_score = model.predict(padded_sequence)[0][0]  # Model predicts a normalized score\n",
        "    final_score = normalized_score * scale_factor  # Scale back to original range (e.g., 0-10)\n",
        "\n",
        "    return final_score\n"
      ],
      "metadata": {
        "id": "2XSVCsMq2H4g"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if __name__ == \"__main__\":\n",
        "  sample_essay = \"\"\"\n",
        "    AI is in education now. It personalizes learning and helps disabled students with tools like screen readers. Teachers don’t have to grade as much because AI does it. But there are problems like privacy and less human interaction. Still, AI will probably stay in education.\n",
        "\"\"\"\n",
        "\n",
        "  score = predict_score(sample_essay, tokenizer, model)\n",
        "  print(f\"Predicted Score: {score}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6X3TcRt22JWQ",
        "outputId": "32735e50-b647-4e9f-da98-d75f6b805a08"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 90ms/step\n",
            "Predicted Score: 4.370096921920776\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "model.save('my_model.keras')"
      ],
      "metadata": {
        "id": "gcCkQayZ7Jh5"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ascAuazB7hd7"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}