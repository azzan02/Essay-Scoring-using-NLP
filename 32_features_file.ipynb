{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 122,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ab6g2uw3jxBy",
        "outputId": "63b36a37-875d-45c1-bfd0-c79dd2b7febb"
      },
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Package punkt_tab is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "import string\n",
        "import nltk\n",
        "\n",
        "# Download necessary NLTK resources\n",
        "nltk.download('punkt') # This downloads the 'punkt' resource needed for word_tokenize\n",
        "nltk.download('punkt_tab')\n",
        "nltk.download('stopwords') # This downloads the missing 'stopwords' resource\n",
        "nltk.download('wordnet') # This downloads the 'wordnet' resource needed for lemmatization\n",
        "\n",
        "# Load dataset\n",
        "essays = pd.read_csv(\"/content/Processed_data.csv\", usecols=[3])  # Read only the 4th column (index 3)\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "# Preprocessing function\n",
        "def preprocess(text):\n",
        "    tokens = word_tokenize(text.lower())\n",
        "    tokens = [word for word in tokens if word not in stopwords.words('english') and word not in string.punctuation]\n",
        "    tokens = [lemmatizer.lemmatize(word) for word in tokens]\n",
        "    return tokens\n",
        "\n",
        "essays['cleaned_text'] = essays['essay'].apply(preprocess)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import spacy\n",
        "import nltk\n",
        "from textblob import TextBlob\n",
        "from collections import Counter\n",
        "import numpy as np\n",
        "from nltk.tokenize import sent_tokenize, word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.sentiment import SentimentIntensityAnalyzer\n",
        "from nltk.tag import pos_tag\n",
        "from tqdm import tqdm\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "class EssayDatasetAnalyzer:\n",
        "    def __init__(self):\n",
        "        # Initialize required NLTK resources\n",
        "        nltk.download('punkt', quiet=True)\n",
        "        nltk.download('averaged_perceptron_tagger', quiet=True)\n",
        "        nltk.download('stopwords', quiet=True)\n",
        "        nltk.download('vader_lexicon', quiet=True)\n",
        "\n",
        "        # Load models and resources\n",
        "        self.nlp = spacy.load('en_core_web_sm')\n",
        "        self.sia = SentimentIntensityAnalyzer()\n",
        "        self.stop_words = set(stopwords.words('english'))\n",
        "\n",
        "    def extract_all_features(self, df):\n",
        "        \"\"\"Extract all features from the dataset\"\"\"\n",
        "        print(\"Extracting features from essays...\")\n",
        "\n",
        "        # Initialize lists to store features\n",
        "        all_features = []\n",
        "\n",
        "        # Process each essay with progress bar\n",
        "        for idx, row in tqdm(df.iterrows(), total=len(df)):\n",
        "            essay_text = str(row['essay'])  # Convert to string to handle any non-string entries\n",
        "\n",
        "            # Extract all features for this essay\n",
        "            features = {\n",
        "                'essay_id': idx,\n",
        "                **self.get_basic_stats(essay_text),\n",
        "                **self.get_vocabulary_features(essay_text),\n",
        "                **self.get_grammar_features(essay_text),\n",
        "                **self.get_sentence_complexity(essay_text),\n",
        "                **self.get_coherence_features(essay_text),\n",
        "                **self.get_style_features(essay_text),\n",
        "                **self.get_argumentation_features(essay_text)\n",
        "            }\n",
        "\n",
        "            all_features.append(features)\n",
        "\n",
        "        # Create features DataFrame\n",
        "        features_df = pd.DataFrame(all_features)\n",
        "\n",
        "        # Add original essay text if needed\n",
        "        features_df['original_essay'] = df['essay']\n",
        "\n",
        "        return features_df\n",
        "\n",
        "    def get_basic_stats(self, text):\n",
        "        \"\"\"Extract basic statistical features\"\"\"\n",
        "        words = word_tokenize(text)\n",
        "        sentences = sent_tokenize(text)\n",
        "        characters = len(text)\n",
        "\n",
        "        return {\n",
        "            'word_count': len(words),\n",
        "            'sentence_count': len(sentences),\n",
        "            'character_count': characters,\n",
        "            'avg_word_per_sentence': len(words) / max(len(sentences), 1),\n",
        "            'avg_char_per_word': characters / max(len(words), 1),\n",
        "            'paragraph_count': len([p for p in text.split('\\n') if p.strip()]),\n",
        "        }\n",
        "\n",
        "    def get_vocabulary_features(self, text):\n",
        "        \"\"\"Extract vocabulary-related features\"\"\"\n",
        "        words = word_tokenize(text.lower())\n",
        "        content_words = [w for w in words if w.isalnum() and w not in self.stop_words]\n",
        "\n",
        "        if not content_words:\n",
        "            return {\n",
        "                'unique_words': 0,\n",
        "                'lexical_diversity': 0,\n",
        "                'avg_word_length': 0,\n",
        "                'long_words_ratio': 0,\n",
        "                'stopwords_ratio': 0\n",
        "            }\n",
        "\n",
        "        return {\n",
        "            'unique_words': len(set(content_words)),\n",
        "            'lexical_diversity': len(set(content_words)) / len(content_words),\n",
        "            'avg_word_length': np.mean([len(word) for word in content_words]),\n",
        "            'long_words_ratio': sum(1 for word in content_words if len(word) > 6) / len(content_words),\n",
        "            'stopwords_ratio': len([w for w in words if w in self.stop_words]) / len(words)\n",
        "        }\n",
        "\n",
        "    def get_grammar_features(self, text):\n",
        "        \"\"\"Extract grammar-related features\"\"\"\n",
        "        doc = self.nlp(text)\n",
        "        total_tokens = len(doc)\n",
        "\n",
        "        if total_tokens == 0:\n",
        "            return {\n",
        "                'noun_ratio': 0,\n",
        "                'verb_ratio': 0,\n",
        "                'adj_ratio': 0,\n",
        "                'adv_ratio': 0,\n",
        "                'pronoun_ratio': 0,\n",
        "                'conjunction_ratio': 0\n",
        "            }\n",
        "\n",
        "        pos_counts = Counter([token.pos_ for token in doc])\n",
        "\n",
        "        return {\n",
        "            'noun_ratio': pos_counts['NOUN'] / total_tokens,\n",
        "            'verb_ratio': pos_counts['VERB'] / total_tokens,\n",
        "            'adj_ratio': pos_counts['ADJ'] / total_tokens,\n",
        "            'adv_ratio': pos_counts['ADV'] / total_tokens,\n",
        "            'pronoun_ratio': pos_counts['PRON'] / total_tokens,\n",
        "            'conjunction_ratio': pos_counts['CCONJ'] / total_tokens\n",
        "        }\n",
        "\n",
        "    def get_sentence_complexity(self, text):\n",
        "        \"\"\"Extract sentence complexity features\"\"\"\n",
        "        doc = self.nlp(text)\n",
        "        sentences = list(doc.sents)\n",
        "\n",
        "        if not sentences:\n",
        "            return {\n",
        "                'avg_sentence_length': 0,\n",
        "                'avg_clauses_per_sentence': 0,\n",
        "                'compound_sentence_ratio': 0,\n",
        "                'complex_sentence_ratio': 0\n",
        "            }\n",
        "\n",
        "        clause_counts = []\n",
        "        compound_sentences = 0\n",
        "        complex_sentences = 0\n",
        "\n",
        "        for sent in sentences:\n",
        "            # Count clauses\n",
        "            clause_count = 1 + len([token for token in sent\n",
        "                                  if token.dep_ in ['advcl', 'ccomp', 'xcomp']])\n",
        "            clause_counts.append(clause_count)\n",
        "\n",
        "            # Check for compound sentences\n",
        "            if any(token.dep_ == 'conj' for token in sent):\n",
        "                compound_sentences += 1\n",
        "\n",
        "            # Check for complex sentences\n",
        "            if clause_count > 1:\n",
        "                complex_sentences += 1\n",
        "\n",
        "        return {\n",
        "            'avg_sentence_length': np.mean([len(sent) for sent in sentences]),\n",
        "            'avg_clauses_per_sentence': np.mean(clause_counts),\n",
        "            'compound_sentence_ratio': compound_sentences / len(sentences),\n",
        "            'complex_sentence_ratio': complex_sentences / len(sentences)\n",
        "        }\n",
        "\n",
        "    def get_coherence_features(self, text):\n",
        "        \"\"\"Extract coherence-related features\"\"\"\n",
        "        sentences = sent_tokenize(text)\n",
        "\n",
        "        if len(sentences) < 2:\n",
        "            return {\n",
        "                'avg_similarity': 0,\n",
        "                'similarity_variance': 0,\n",
        "                'transition_words_ratio': 0\n",
        "            }\n",
        "\n",
        "        # Analyze sentence embeddings similarity\n",
        "        sent_embeddings = [self.nlp(sent).vector for sent in sentences]\n",
        "        similarities = []\n",
        "        for i in range(len(sent_embeddings)-1):\n",
        "            similarity = np.dot(sent_embeddings[i], sent_embeddings[i+1])\n",
        "            similarities.append(float(similarity))\n",
        "\n",
        "        # Count transition words\n",
        "        transition_words = set(['however', 'therefore', 'furthermore', 'moreover',\n",
        "                              'nevertheless', 'thus', 'meanwhile', 'consequently',\n",
        "                              'similarly', 'in contrast', 'additionally'])\n",
        "\n",
        "        words = word_tokenize(text.lower())\n",
        "        transition_count = sum(1 for word in words if word in transition_words)\n",
        "\n",
        "        return {\n",
        "            'avg_similarity': np.mean(similarities),\n",
        "            'similarity_variance': np.var(similarities),\n",
        "            'transition_words_ratio': transition_count / len(words)\n",
        "        }\n",
        "\n",
        "    def get_style_features(self, text):\n",
        "        \"\"\"Extract style-related features\"\"\"\n",
        "        sentiment = self.sia.polarity_scores(text)\n",
        "        doc = self.nlp(text)\n",
        "\n",
        "        # Calculate formality\n",
        "        formal_indicators = len([token for token in doc\n",
        "                               if token.pos_ in ['NOUN', 'ADJ', 'NUM']])\n",
        "        informal_indicators = len([token for token in doc\n",
        "                                 if token.pos_ in ['INTJ', 'PART']])\n",
        "\n",
        "        return {\n",
        "            'sentiment_positive': sentiment['pos'],\n",
        "            'sentiment_negative': sentiment['neg'],\n",
        "            'sentiment_neutral': sentiment['neu'],\n",
        "            'sentiment_compound': sentiment['compound'],\n",
        "            'formality_score': formal_indicators / (informal_indicators + 1)\n",
        "        }\n",
        "\n",
        "    def get_argumentation_features(self, text):\n",
        "        \"\"\"Extract argumentation-related features\"\"\"\n",
        "        text_lower = text.lower()\n",
        "        words = word_tokenize(text_lower)\n",
        "        total_words = len(words)\n",
        "\n",
        "        if total_words == 0:\n",
        "            return {\n",
        "                'claims_ratio': 0,\n",
        "                'evidence_ratio': 0,\n",
        "                'counter_argument_ratio': 0\n",
        "            }\n",
        "\n",
        "        # Count argument markers\n",
        "        argument_markers = {\n",
        "            'claims': ['argue', 'claim', 'believe', 'suggest', 'think', 'conclude'],\n",
        "            'evidence': ['because', 'since', 'therefore', 'consequently', 'research', 'study', 'evidence'],\n",
        "            'counter_arguments': ['however', 'although', 'nevertheless', 'despite', 'contrary', 'whereas']\n",
        "        }\n",
        "\n",
        "        counts = {\n",
        "            f'{key}_ratio': sum(1 for marker in markers\n",
        "                               if marker in text_lower) / total_words\n",
        "            for key, markers in argument_markers.items()\n",
        "        }\n",
        "\n",
        "        return counts\n",
        "\n",
        "def analyze_essay_dataset(input_df):\n",
        "    \"\"\"Main function to analyze the essay dataset\"\"\"\n",
        "    # Create analyzer instance\n",
        "    analyzer = EssayDatasetAnalyzer()\n",
        "\n",
        "    # Extract features\n",
        "    features_df = analyzer.extract_all_features(input_df)\n",
        "\n",
        "    # Basic statistics of features\n",
        "    feature_stats = features_df.describe()\n",
        "\n",
        "    return features_df, feature_stats\n",
        "\n",
        "# Example usage\n",
        "# Read your dataset\n",
        "df = pd.read_csv('/content/Processed_data.csv')\n",
        "\n",
        "# Analyze essays\n",
        "features_df, feature_stats = analyze_essay_dataset(df)\n",
        "\n",
        "# Save results\n",
        "features_df.to_csv('essay_features.csv', index=False)\n",
        "feature_stats.to_csv('feature_statistics.csv')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uJ0qHyBVgIM9",
        "outputId": "5aa810f0-c92a-48e2-e050-63dad5701b22"
      },
      "execution_count": 137,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting features from essays...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 12976/12976 [56:49<00:00,  3.81it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Load the dataframe\n",
        "df2 = pd.read_csv('/content/Processed_data.csv')\n",
        "features_df = pd.read_csv('/content/essay_features.csv')\n",
        "\n",
        "# Print some info\n",
        "print(df2.head())\n",
        "print(df2.info())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BmT9E287utPI",
        "outputId": "8f1c99d7-bdb6-49c2-8420-8c1a7a038d2b"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   Unnamed: 0  essay_id  essay_set  \\\n",
            "0           0         1          1   \n",
            "1           1         2          1   \n",
            "2           2         3          1   \n",
            "3           3         4          1   \n",
            "4           4         5          1   \n",
            "\n",
            "                                               essay  final_score  \\\n",
            "0  Dear local newspaper, I think effects computer...            6   \n",
            "1  Dear I believe that using computers will benef...            7   \n",
            "2  Dear, More and more people use computers, but ...            5   \n",
            "3  Dear Local Newspaper, I have found that many e...            8   \n",
            "4  Dear I know having computers has a positive ef...            6   \n",
            "\n",
            "                                         clean_essay  char_count  word_count  \\\n",
            "0  Dear local newspaper  I think effects computer...        1441         344   \n",
            "1  Dear I believe using computers benefit us many...        1765         413   \n",
            "2  Dear  More people use computers  everyone agre...        1185         276   \n",
            "3  Dear Local Newspaper  I found many experts say...        2284         490   \n",
            "4  Dear I know computers positive effect people  ...        2023         469   \n",
            "\n",
            "   sent_count  avg_word_len  spell_err_count  noun_count  adj_count  \\\n",
            "0          16      4.188953               11          76         75   \n",
            "1          17      4.273608               21          98         84   \n",
            "2          14      4.293478                5          76         51   \n",
            "3          26      4.661224               31         142         96   \n",
            "4          30      4.313433               18         110         90   \n",
            "\n",
            "   verb_count  adv_count  \n",
            "0          18         24  \n",
            "1          20         19  \n",
            "2          20         16  \n",
            "3          39         29  \n",
            "4          32         36  \n",
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 12976 entries, 0 to 12975\n",
            "Data columns (total 15 columns):\n",
            " #   Column           Non-Null Count  Dtype  \n",
            "---  ------           --------------  -----  \n",
            " 0   Unnamed: 0       12976 non-null  int64  \n",
            " 1   essay_id         12976 non-null  int64  \n",
            " 2   essay_set        12976 non-null  int64  \n",
            " 3   essay            12976 non-null  object \n",
            " 4   final_score      12976 non-null  int64  \n",
            " 5   clean_essay      12976 non-null  object \n",
            " 6   char_count       12976 non-null  int64  \n",
            " 7   word_count       12976 non-null  int64  \n",
            " 8   sent_count       12976 non-null  int64  \n",
            " 9   avg_word_len     12976 non-null  float64\n",
            " 10  spell_err_count  12976 non-null  int64  \n",
            " 11  noun_count       12976 non-null  int64  \n",
            " 12  adj_count        12976 non-null  int64  \n",
            " 13  verb_count       12976 non-null  int64  \n",
            " 14  adv_count        12976 non-null  int64  \n",
            "dtypes: float64(1), int64(12), object(2)\n",
            "memory usage: 1.5+ MB\n",
            "None\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Assuming 'final_score' column exists in df2 and has the same number of rows as df\n",
        "features_df['final_score'] = df2['final_score']"
      ],
      "metadata": {
        "id": "Pwqd6jkSu45O"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "features_df.head()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 516
        },
        "id": "C_C3WGCFu-Xg",
        "outputId": "9c5aa333-6992-4ed3-aa75-f1f42b85c7b3"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "   essay_id  word_count  sentence_count  character_count  \\\n",
              "0         0         372              16             1819   \n",
              "1         1         440              17             2205   \n",
              "2         2         298              14             1482   \n",
              "3         3         530              26             2817   \n",
              "4         4         508              30             2533   \n",
              "\n",
              "   avg_word_per_sentence  avg_char_per_word  paragraph_count  unique_words  \\\n",
              "0              23.250000           4.889785                1           100   \n",
              "1              25.882353           5.011364                1           126   \n",
              "2              21.285714           4.973154                1            90   \n",
              "3              20.384615           5.315094                1           170   \n",
              "4              16.933333           4.986220                1           129   \n",
              "\n",
              "   lexical_diversity  avg_word_length  ...  sentiment_positive  \\\n",
              "0           0.666667         5.526667  ...               0.172   \n",
              "1           0.591549         5.661972  ...               0.224   \n",
              "2           0.697674         6.069767  ...               0.201   \n",
              "3           0.658915         6.244186  ...               0.163   \n",
              "4           0.594470         5.917051  ...               0.097   \n",
              "\n",
              "   sentiment_negative  sentiment_neutral  sentiment_compound  formality_score  \\\n",
              "0               0.000              0.828              0.9954         5.166667   \n",
              "1               0.015              0.761              0.9983        10.333333   \n",
              "2               0.046              0.753              0.9947         8.363636   \n",
              "3               0.008              0.829              0.9980         7.304348   \n",
              "4               0.026              0.878              0.9776         7.100000   \n",
              "\n",
              "   claims_ratio  evidence_ratio  counter_arguments_ratio  \\\n",
              "0      0.005376        0.002688                      0.0   \n",
              "1      0.002273        0.002273                      0.0   \n",
              "2      0.006711        0.000000                      0.0   \n",
              "3      0.000000        0.003774                      0.0   \n",
              "4      0.000000        0.001969                      0.0   \n",
              "\n",
              "                                      original_essay  final_score  \n",
              "0  Dear local newspaper, I think effects computer...            6  \n",
              "1  Dear I believe that using computers will benef...            7  \n",
              "2  Dear, More and more people use computers, but ...            5  \n",
              "3  Dear Local Newspaper, I have found that many e...            8  \n",
              "4  Dear I know having computers has a positive ef...            6  \n",
              "\n",
              "[5 rows x 35 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-1b8abcbb-3bb5-4e12-95a9-e3244a7c76e1\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>essay_id</th>\n",
              "      <th>word_count</th>\n",
              "      <th>sentence_count</th>\n",
              "      <th>character_count</th>\n",
              "      <th>avg_word_per_sentence</th>\n",
              "      <th>avg_char_per_word</th>\n",
              "      <th>paragraph_count</th>\n",
              "      <th>unique_words</th>\n",
              "      <th>lexical_diversity</th>\n",
              "      <th>avg_word_length</th>\n",
              "      <th>...</th>\n",
              "      <th>sentiment_positive</th>\n",
              "      <th>sentiment_negative</th>\n",
              "      <th>sentiment_neutral</th>\n",
              "      <th>sentiment_compound</th>\n",
              "      <th>formality_score</th>\n",
              "      <th>claims_ratio</th>\n",
              "      <th>evidence_ratio</th>\n",
              "      <th>counter_arguments_ratio</th>\n",
              "      <th>original_essay</th>\n",
              "      <th>final_score</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>372</td>\n",
              "      <td>16</td>\n",
              "      <td>1819</td>\n",
              "      <td>23.250000</td>\n",
              "      <td>4.889785</td>\n",
              "      <td>1</td>\n",
              "      <td>100</td>\n",
              "      <td>0.666667</td>\n",
              "      <td>5.526667</td>\n",
              "      <td>...</td>\n",
              "      <td>0.172</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.828</td>\n",
              "      <td>0.9954</td>\n",
              "      <td>5.166667</td>\n",
              "      <td>0.005376</td>\n",
              "      <td>0.002688</td>\n",
              "      <td>0.0</td>\n",
              "      <td>Dear local newspaper, I think effects computer...</td>\n",
              "      <td>6</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>440</td>\n",
              "      <td>17</td>\n",
              "      <td>2205</td>\n",
              "      <td>25.882353</td>\n",
              "      <td>5.011364</td>\n",
              "      <td>1</td>\n",
              "      <td>126</td>\n",
              "      <td>0.591549</td>\n",
              "      <td>5.661972</td>\n",
              "      <td>...</td>\n",
              "      <td>0.224</td>\n",
              "      <td>0.015</td>\n",
              "      <td>0.761</td>\n",
              "      <td>0.9983</td>\n",
              "      <td>10.333333</td>\n",
              "      <td>0.002273</td>\n",
              "      <td>0.002273</td>\n",
              "      <td>0.0</td>\n",
              "      <td>Dear I believe that using computers will benef...</td>\n",
              "      <td>7</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2</td>\n",
              "      <td>298</td>\n",
              "      <td>14</td>\n",
              "      <td>1482</td>\n",
              "      <td>21.285714</td>\n",
              "      <td>4.973154</td>\n",
              "      <td>1</td>\n",
              "      <td>90</td>\n",
              "      <td>0.697674</td>\n",
              "      <td>6.069767</td>\n",
              "      <td>...</td>\n",
              "      <td>0.201</td>\n",
              "      <td>0.046</td>\n",
              "      <td>0.753</td>\n",
              "      <td>0.9947</td>\n",
              "      <td>8.363636</td>\n",
              "      <td>0.006711</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>Dear, More and more people use computers, but ...</td>\n",
              "      <td>5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>3</td>\n",
              "      <td>530</td>\n",
              "      <td>26</td>\n",
              "      <td>2817</td>\n",
              "      <td>20.384615</td>\n",
              "      <td>5.315094</td>\n",
              "      <td>1</td>\n",
              "      <td>170</td>\n",
              "      <td>0.658915</td>\n",
              "      <td>6.244186</td>\n",
              "      <td>...</td>\n",
              "      <td>0.163</td>\n",
              "      <td>0.008</td>\n",
              "      <td>0.829</td>\n",
              "      <td>0.9980</td>\n",
              "      <td>7.304348</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.003774</td>\n",
              "      <td>0.0</td>\n",
              "      <td>Dear Local Newspaper, I have found that many e...</td>\n",
              "      <td>8</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>4</td>\n",
              "      <td>508</td>\n",
              "      <td>30</td>\n",
              "      <td>2533</td>\n",
              "      <td>16.933333</td>\n",
              "      <td>4.986220</td>\n",
              "      <td>1</td>\n",
              "      <td>129</td>\n",
              "      <td>0.594470</td>\n",
              "      <td>5.917051</td>\n",
              "      <td>...</td>\n",
              "      <td>0.097</td>\n",
              "      <td>0.026</td>\n",
              "      <td>0.878</td>\n",
              "      <td>0.9776</td>\n",
              "      <td>7.100000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.001969</td>\n",
              "      <td>0.0</td>\n",
              "      <td>Dear I know having computers has a positive ef...</td>\n",
              "      <td>6</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5 rows × 35 columns</p>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-1b8abcbb-3bb5-4e12-95a9-e3244a7c76e1')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-1b8abcbb-3bb5-4e12-95a9-e3244a7c76e1 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-1b8abcbb-3bb5-4e12-95a9-e3244a7c76e1');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-614c0c9f-7d65-4343-a85d-1d716b2c7510\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-614c0c9f-7d65-4343-a85d-1d716b2c7510')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-614c0c9f-7d65-4343-a85d-1d716b2c7510 button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "features_df"
            }
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Traiing the model"
      ],
      "metadata": {
        "id": "uf4t5OarCv0D"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "import numpy as np"
      ],
      "metadata": {
        "id": "Bug9mvgUCyRw"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X = features_df.drop(columns=['essay_id','original_essay',\t'final_score']).values\n",
        "y = features_df['final_score'].values"
      ],
      "metadata": {
        "id": "Im3Ao2TzS0gl"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pXt6GVUTVB89",
        "outputId": "c3ef478d-ba8a-45ea-e348-2147975c3006"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(12976, 32)"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Split data into features (X) and target (y)\n",
        "\n",
        "# Split into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Scale the features\n",
        "scaler = StandardScaler()\n",
        "X_train = scaler.fit_transform(X_train)\n",
        "X_test = scaler.transform(X_test)"
      ],
      "metadata": {
        "id": "oIcEHCQaGATP"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "# Check for NaNs\n",
        "print(np.any(np.isnan(X_train)), np.any(np.isnan(y_train)))\n",
        "\n",
        "# Check for infinite values\n",
        "print(np.any(np.isinf(X_train)), np.any(np.isinf(y_train)))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RezO8UMcI90Y",
        "outputId": "4d44b497-6aaf-4aae-9e83-2e15a09fff43"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "False False\n",
            "False False\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Replace NaNs with 0 or the mean of the column\n",
        "X_train = np.nan_to_num(X_train, nan=0.0)\n",
        "y_train = np.nan_to_num(y_train, nan=0.0)\n"
      ],
      "metadata": {
        "id": "Ag5qpSwLJGxV"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train[0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UViVGbzoSKej",
        "outputId": "0929c06c-a497-4aec-e3b9-6e95de613f0b"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([ 1.17840347,  1.37396598,  1.26483133, -0.40920389,  0.27063426,\n",
              "        0.        ,  0.29760402, -2.26959366,  0.23002019,  0.16870968,\n",
              "        1.09444117,  0.47008388, -0.111486  ,  0.16092803, -1.32506706,\n",
              "        0.10251776, -0.43632261, -0.39865283, -0.39430581, -0.24035032,\n",
              "       -0.37122466,  0.04083269, -0.08499248, -0.24941606, -0.28848892,\n",
              "        0.27352331,  0.11177191,  0.09697399, -0.49678403, -0.16392111,\n",
              "       -0.32664153, -0.2538463 ])"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"y_train min:\", y_train.min())\n",
        "print(\"y_train max:\", y_train.max())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7k6bqMJkJMxL",
        "outputId": "8e1380e3-08b2-4785-c0aa-34e360786838"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "y_train min: 0\n",
            "y_train max: 10\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.layers import Dropout\n",
        "\n",
        "# Build the neural network model\n",
        "model = Sequential([\n",
        "    Dense(128, input_dim=X_train.shape[1], activation='relu'),\n",
        "    Dropout(0.1),\n",
        "    Dense(64, activation='sigmoid'),\n",
        "    Dropout(0.1),\n",
        "    Dense(32, activation='relu'),\n",
        "    Dense(1)\n",
        "])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jme-ReqEGIYj",
        "outputId": "4fe42d49-a476-46b3-a2fe-fccb770a6e0e"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/layers/core/dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.models import load_model\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint\n",
        "\n",
        "# Compile the model\n",
        "model.compile(optimizer=Adam(learning_rate=0.0005), loss='mse', metrics=['mae', 'mse'])\n",
        "\n",
        "# Set up ModelCheckpoint to save the best model during training\n",
        "checkpoint = ModelCheckpoint(\n",
        "    'best_model.keras',  # Filepath to save the model\n",
        "    monitor='val_loss',  # Metric to monitor\n",
        "    save_best_only=True,  # Save only the best model\n",
        "    mode='min',  # Minimize the monitored metric\n",
        "    verbose=1\n",
        ")\n",
        "# Train the model\n",
        "history = model.fit(X_train, y_train, validation_split=0.2, epochs=50, batch_size=16, verbose=1, callbacks=[checkpoint])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L-90ZyOnGLaL",
        "outputId": "22647245-3b37-4387-f75a-4ad53bd5e649"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/50\n",
            "\u001b[1m500/519\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 14.2112 - mae: 2.9766 - mse: 14.2112\n",
            "Epoch 1: val_loss improved from inf to 4.29307, saving model to best_model.keras\n",
            "\u001b[1m519/519\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - loss: 13.9589 - mae: 2.9442 - mse: 13.9589 - val_loss: 4.2931 - val_mae: 1.6389 - val_mse: 4.2931\n",
            "Epoch 2/50\n",
            "\u001b[1m509/519\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 4.0623 - mae: 1.5727 - mse: 4.0623\n",
            "Epoch 2: val_loss improved from 4.29307 to 3.42081, saving model to best_model.keras\n",
            "\u001b[1m519/519\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 4.0584 - mae: 1.5721 - mse: 4.0584 - val_loss: 3.4208 - val_mae: 1.4536 - val_mse: 3.4208\n",
            "Epoch 3/50\n",
            "\u001b[1m517/519\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 3.5774 - mae: 1.4767 - mse: 3.5774\n",
            "Epoch 3: val_loss improved from 3.42081 to 3.12990, saving model to best_model.keras\n",
            "\u001b[1m519/519\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 3.5764 - mae: 1.4765 - mse: 3.5764 - val_loss: 3.1299 - val_mae: 1.3844 - val_mse: 3.1299\n",
            "Epoch 4/50\n",
            "\u001b[1m502/519\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 3.1175 - mae: 1.3628 - mse: 3.1175\n",
            "Epoch 4: val_loss improved from 3.12990 to 3.04402, saving model to best_model.keras\n",
            "\u001b[1m519/519\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 3.1199 - mae: 1.3636 - mse: 3.1199 - val_loss: 3.0440 - val_mae: 1.3630 - val_mse: 3.0440\n",
            "Epoch 5/50\n",
            "\u001b[1m503/519\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 2.9913 - mae: 1.3496 - mse: 2.9913\n",
            "Epoch 5: val_loss improved from 3.04402 to 2.94219, saving model to best_model.keras\n",
            "\u001b[1m519/519\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 2.9934 - mae: 1.3500 - mse: 2.9934 - val_loss: 2.9422 - val_mae: 1.3262 - val_mse: 2.9422\n",
            "Epoch 6/50\n",
            "\u001b[1m508/519\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 3.0400 - mae: 1.3557 - mse: 3.0400\n",
            "Epoch 6: val_loss improved from 2.94219 to 2.89366, saving model to best_model.keras\n",
            "\u001b[1m519/519\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 3.0387 - mae: 1.3554 - mse: 3.0387 - val_loss: 2.8937 - val_mae: 1.3111 - val_mse: 2.8937\n",
            "Epoch 7/50\n",
            "\u001b[1m500/519\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 2.9755 - mae: 1.3381 - mse: 2.9755\n",
            "Epoch 7: val_loss improved from 2.89366 to 2.86259, saving model to best_model.keras\n",
            "\u001b[1m519/519\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 2.9743 - mae: 1.3377 - mse: 2.9743 - val_loss: 2.8626 - val_mae: 1.3095 - val_mse: 2.8626\n",
            "Epoch 8/50\n",
            "\u001b[1m516/519\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 2.8600 - mae: 1.3037 - mse: 2.8600\n",
            "Epoch 8: val_loss did not improve from 2.86259\n",
            "\u001b[1m519/519\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - loss: 2.8602 - mae: 1.3037 - mse: 2.8602 - val_loss: 2.8877 - val_mae: 1.3107 - val_mse: 2.8877\n",
            "Epoch 9/50\n",
            "\u001b[1m502/519\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 2.8179 - mae: 1.2947 - mse: 2.8179\n",
            "Epoch 9: val_loss did not improve from 2.86259\n",
            "\u001b[1m519/519\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - loss: 2.8187 - mae: 1.2949 - mse: 2.8187 - val_loss: 2.8873 - val_mae: 1.3050 - val_mse: 2.8873\n",
            "Epoch 10/50\n",
            "\u001b[1m498/519\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 2.7628 - mae: 1.2814 - mse: 2.7628\n",
            "Epoch 10: val_loss improved from 2.86259 to 2.83105, saving model to best_model.keras\n",
            "\u001b[1m519/519\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - loss: 2.7643 - mae: 1.2817 - mse: 2.7643 - val_loss: 2.8311 - val_mae: 1.2966 - val_mse: 2.8311\n",
            "Epoch 11/50\n",
            "\u001b[1m498/519\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 2.8360 - mae: 1.2919 - mse: 2.8360\n",
            "Epoch 11: val_loss improved from 2.83105 to 2.81793, saving model to best_model.keras\n",
            "\u001b[1m519/519\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 2.8350 - mae: 1.2918 - mse: 2.8350 - val_loss: 2.8179 - val_mae: 1.2911 - val_mse: 2.8179\n",
            "Epoch 12/50\n",
            "\u001b[1m504/519\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 2.7412 - mae: 1.2751 - mse: 2.7412\n",
            "Epoch 12: val_loss improved from 2.81793 to 2.78620, saving model to best_model.keras\n",
            "\u001b[1m519/519\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 2.7407 - mae: 1.2751 - mse: 2.7407 - val_loss: 2.7862 - val_mae: 1.2785 - val_mse: 2.7862\n",
            "Epoch 13/50\n",
            "\u001b[1m498/519\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 2.6380 - mae: 1.2491 - mse: 2.6380\n",
            "Epoch 13: val_loss did not improve from 2.78620\n",
            "\u001b[1m519/519\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 2.6407 - mae: 1.2497 - mse: 2.6407 - val_loss: 2.8532 - val_mae: 1.2883 - val_mse: 2.8532\n",
            "Epoch 14/50\n",
            "\u001b[1m500/519\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 2.6613 - mae: 1.2517 - mse: 2.6613\n",
            "Epoch 14: val_loss improved from 2.78620 to 2.78356, saving model to best_model.keras\n",
            "\u001b[1m519/519\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 2.6620 - mae: 1.2519 - mse: 2.6620 - val_loss: 2.7836 - val_mae: 1.2857 - val_mse: 2.7836\n",
            "Epoch 15/50\n",
            "\u001b[1m490/519\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 2.6618 - mae: 1.2601 - mse: 2.6618\n",
            "Epoch 15: val_loss improved from 2.78356 to 2.76695, saving model to best_model.keras\n",
            "\u001b[1m519/519\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 2.6612 - mae: 1.2596 - mse: 2.6612 - val_loss: 2.7669 - val_mae: 1.2684 - val_mse: 2.7669\n",
            "Epoch 16/50\n",
            "\u001b[1m491/519\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 2.6484 - mae: 1.2552 - mse: 2.6484\n",
            "Epoch 16: val_loss did not improve from 2.76695\n",
            "\u001b[1m519/519\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 2.6479 - mae: 1.2549 - mse: 2.6479 - val_loss: 2.7676 - val_mae: 1.2682 - val_mse: 2.7676\n",
            "Epoch 17/50\n",
            "\u001b[1m514/519\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 2.5955 - mae: 1.2415 - mse: 2.5955\n",
            "Epoch 17: val_loss did not improve from 2.76695\n",
            "\u001b[1m519/519\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - loss: 2.5957 - mae: 1.2415 - mse: 2.5957 - val_loss: 2.7884 - val_mae: 1.2777 - val_mse: 2.7884\n",
            "Epoch 18/50\n",
            "\u001b[1m509/519\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 2.5734 - mae: 1.2200 - mse: 2.5734\n",
            "Epoch 18: val_loss did not improve from 2.76695\n",
            "\u001b[1m519/519\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - loss: 2.5744 - mae: 1.2204 - mse: 2.5744 - val_loss: 2.8177 - val_mae: 1.2864 - val_mse: 2.8177\n",
            "Epoch 19/50\n",
            "\u001b[1m488/519\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 2.5541 - mae: 1.2302 - mse: 2.5541\n",
            "Epoch 19: val_loss improved from 2.76695 to 2.75999, saving model to best_model.keras\n",
            "\u001b[1m519/519\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 2.5549 - mae: 1.2300 - mse: 2.5549 - val_loss: 2.7600 - val_mae: 1.2671 - val_mse: 2.7600\n",
            "Epoch 20/50\n",
            "\u001b[1m492/519\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 2.6621 - mae: 1.2468 - mse: 2.6621\n",
            "Epoch 20: val_loss improved from 2.75999 to 2.74463, saving model to best_model.keras\n",
            "\u001b[1m519/519\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - loss: 2.6588 - mae: 1.2462 - mse: 2.6588 - val_loss: 2.7446 - val_mae: 1.2571 - val_mse: 2.7446\n",
            "Epoch 21/50\n",
            "\u001b[1m491/519\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 2.5562 - mae: 1.2244 - mse: 2.5562\n",
            "Epoch 21: val_loss did not improve from 2.74463\n",
            "\u001b[1m519/519\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 2.5559 - mae: 1.2242 - mse: 2.5559 - val_loss: 2.7476 - val_mae: 1.2615 - val_mse: 2.7476\n",
            "Epoch 22/50\n",
            "\u001b[1m500/519\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 2.5766 - mae: 1.2288 - mse: 2.5766\n",
            "Epoch 22: val_loss did not improve from 2.74463\n",
            "\u001b[1m519/519\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 2.5760 - mae: 1.2288 - mse: 2.5760 - val_loss: 2.7637 - val_mae: 1.2539 - val_mse: 2.7637\n",
            "Epoch 23/50\n",
            "\u001b[1m496/519\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 2.5279 - mae: 1.2113 - mse: 2.5279\n",
            "Epoch 23: val_loss improved from 2.74463 to 2.74340, saving model to best_model.keras\n",
            "\u001b[1m519/519\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 2.5266 - mae: 1.2112 - mse: 2.5266 - val_loss: 2.7434 - val_mae: 1.2633 - val_mse: 2.7434\n",
            "Epoch 24/50\n",
            "\u001b[1m519/519\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 2.5071 - mae: 1.2229 - mse: 2.5071\n",
            "Epoch 24: val_loss improved from 2.74340 to 2.71576, saving model to best_model.keras\n",
            "\u001b[1m519/519\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 2.5071 - mae: 1.2228 - mse: 2.5071 - val_loss: 2.7158 - val_mae: 1.2540 - val_mse: 2.7158\n",
            "Epoch 25/50\n",
            "\u001b[1m498/519\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 2.5236 - mae: 1.2142 - mse: 2.5236\n",
            "Epoch 25: val_loss did not improve from 2.71576\n",
            "\u001b[1m519/519\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 2.5233 - mae: 1.2142 - mse: 2.5233 - val_loss: 2.7260 - val_mae: 1.2515 - val_mse: 2.7260\n",
            "Epoch 26/50\n",
            "\u001b[1m518/519\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 2.5007 - mae: 1.2155 - mse: 2.5007\n",
            "Epoch 26: val_loss did not improve from 2.71576\n",
            "\u001b[1m519/519\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 2.5006 - mae: 1.2155 - mse: 2.5006 - val_loss: 2.7280 - val_mae: 1.2606 - val_mse: 2.7280\n",
            "Epoch 27/50\n",
            "\u001b[1m508/519\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 2.4175 - mae: 1.1909 - mse: 2.4175\n",
            "Epoch 27: val_loss did not improve from 2.71576\n",
            "\u001b[1m519/519\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - loss: 2.4186 - mae: 1.1912 - mse: 2.4186 - val_loss: 2.7432 - val_mae: 1.2501 - val_mse: 2.7432\n",
            "Epoch 28/50\n",
            "\u001b[1m493/519\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 2.4487 - mae: 1.1967 - mse: 2.4487\n",
            "Epoch 28: val_loss did not improve from 2.71576\n",
            "\u001b[1m519/519\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - loss: 2.4501 - mae: 1.1971 - mse: 2.4501 - val_loss: 2.8108 - val_mae: 1.2759 - val_mse: 2.8108\n",
            "Epoch 29/50\n",
            "\u001b[1m494/519\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 2.5225 - mae: 1.2200 - mse: 2.5225\n",
            "Epoch 29: val_loss did not improve from 2.71576\n",
            "\u001b[1m519/519\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 2.5195 - mae: 1.2191 - mse: 2.5195 - val_loss: 2.7253 - val_mae: 1.2402 - val_mse: 2.7253\n",
            "Epoch 30/50\n",
            "\u001b[1m501/519\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 2.4033 - mae: 1.1852 - mse: 2.4033\n",
            "Epoch 30: val_loss did not improve from 2.71576\n",
            "\u001b[1m519/519\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 2.4042 - mae: 1.1854 - mse: 2.4042 - val_loss: 2.7307 - val_mae: 1.2493 - val_mse: 2.7307\n",
            "Epoch 31/50\n",
            "\u001b[1m519/519\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 2.3901 - mae: 1.1862 - mse: 2.3901\n",
            "Epoch 31: val_loss did not improve from 2.71576\n",
            "\u001b[1m519/519\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 2.3901 - mae: 1.1862 - mse: 2.3901 - val_loss: 2.7181 - val_mae: 1.2476 - val_mse: 2.7181\n",
            "Epoch 32/50\n",
            "\u001b[1m508/519\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 2.4429 - mae: 1.1934 - mse: 2.4429\n",
            "Epoch 32: val_loss did not improve from 2.71576\n",
            "\u001b[1m519/519\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 2.4425 - mae: 1.1934 - mse: 2.4425 - val_loss: 2.7565 - val_mae: 1.2538 - val_mse: 2.7565\n",
            "Epoch 33/50\n",
            "\u001b[1m519/519\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 2.3938 - mae: 1.1878 - mse: 2.3938\n",
            "Epoch 33: val_loss did not improve from 2.71576\n",
            "\u001b[1m519/519\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 2.3938 - mae: 1.1878 - mse: 2.3938 - val_loss: 2.7419 - val_mae: 1.2469 - val_mse: 2.7419\n",
            "Epoch 34/50\n",
            "\u001b[1m517/519\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 2.3410 - mae: 1.1783 - mse: 2.3410\n",
            "Epoch 34: val_loss did not improve from 2.71576\n",
            "\u001b[1m519/519\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 2.3412 - mae: 1.1784 - mse: 2.3412 - val_loss: 2.7356 - val_mae: 1.2540 - val_mse: 2.7356\n",
            "Epoch 35/50\n",
            "\u001b[1m518/519\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 2.3105 - mae: 1.1636 - mse: 2.3105\n",
            "Epoch 35: val_loss did not improve from 2.71576\n",
            "\u001b[1m519/519\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 2.3108 - mae: 1.1637 - mse: 2.3108 - val_loss: 2.7805 - val_mae: 1.2646 - val_mse: 2.7805\n",
            "Epoch 36/50\n",
            "\u001b[1m511/519\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 2.2946 - mae: 1.1660 - mse: 2.2946\n",
            "Epoch 36: val_loss did not improve from 2.71576\n",
            "\u001b[1m519/519\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 2.2953 - mae: 1.1661 - mse: 2.2953 - val_loss: 2.7572 - val_mae: 1.2578 - val_mse: 2.7572\n",
            "Epoch 37/50\n",
            "\u001b[1m509/519\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 2.3058 - mae: 1.1552 - mse: 2.3058\n",
            "Epoch 37: val_loss did not improve from 2.71576\n",
            "\u001b[1m519/519\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - loss: 2.3068 - mae: 1.1555 - mse: 2.3068 - val_loss: 2.8165 - val_mae: 1.2754 - val_mse: 2.8165\n",
            "Epoch 38/50\n",
            "\u001b[1m502/519\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 2.3042 - mae: 1.1760 - mse: 2.3042\n",
            "Epoch 38: val_loss did not improve from 2.71576\n",
            "\u001b[1m519/519\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - loss: 2.3049 - mae: 1.1759 - mse: 2.3049 - val_loss: 2.8032 - val_mae: 1.2623 - val_mse: 2.8032\n",
            "Epoch 39/50\n",
            "\u001b[1m515/519\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 2.3722 - mae: 1.1740 - mse: 2.3722\n",
            "Epoch 39: val_loss did not improve from 2.71576\n",
            "\u001b[1m519/519\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - loss: 2.3717 - mae: 1.1739 - mse: 2.3717 - val_loss: 2.7804 - val_mae: 1.2495 - val_mse: 2.7804\n",
            "Epoch 40/50\n",
            "\u001b[1m507/519\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 2.2941 - mae: 1.1547 - mse: 2.2941\n",
            "Epoch 40: val_loss did not improve from 2.71576\n",
            "\u001b[1m519/519\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 2.2950 - mae: 1.1550 - mse: 2.2950 - val_loss: 2.7522 - val_mae: 1.2509 - val_mse: 2.7522\n",
            "Epoch 41/50\n",
            "\u001b[1m519/519\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 2.2780 - mae: 1.1529 - mse: 2.2780\n",
            "Epoch 41: val_loss did not improve from 2.71576\n",
            "\u001b[1m519/519\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 2.2781 - mae: 1.1529 - mse: 2.2781 - val_loss: 2.7473 - val_mae: 1.2430 - val_mse: 2.7473\n",
            "Epoch 42/50\n",
            "\u001b[1m514/519\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 2.2654 - mae: 1.1507 - mse: 2.2654\n",
            "Epoch 42: val_loss did not improve from 2.71576\n",
            "\u001b[1m519/519\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 2.2659 - mae: 1.1508 - mse: 2.2659 - val_loss: 2.8638 - val_mae: 1.2850 - val_mse: 2.8638\n",
            "Epoch 43/50\n",
            "\u001b[1m516/519\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 2.3371 - mae: 1.1625 - mse: 2.3371\n",
            "Epoch 43: val_loss did not improve from 2.71576\n",
            "\u001b[1m519/519\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - loss: 2.3367 - mae: 1.1624 - mse: 2.3367 - val_loss: 2.8093 - val_mae: 1.2627 - val_mse: 2.8093\n",
            "Epoch 44/50\n",
            "\u001b[1m498/519\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 2.3340 - mae: 1.1691 - mse: 2.3340\n",
            "Epoch 44: val_loss did not improve from 2.71576\n",
            "\u001b[1m519/519\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 2.3327 - mae: 1.1688 - mse: 2.3327 - val_loss: 2.8009 - val_mae: 1.2639 - val_mse: 2.8009\n",
            "Epoch 45/50\n",
            "\u001b[1m507/519\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 2.2224 - mae: 1.1395 - mse: 2.2224\n",
            "Epoch 45: val_loss did not improve from 2.71576\n",
            "\u001b[1m519/519\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 2.2232 - mae: 1.1398 - mse: 2.2232 - val_loss: 2.7582 - val_mae: 1.2529 - val_mse: 2.7582\n",
            "Epoch 46/50\n",
            "\u001b[1m507/519\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 2.2339 - mae: 1.1450 - mse: 2.2339\n",
            "Epoch 46: val_loss did not improve from 2.71576\n",
            "\u001b[1m519/519\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - loss: 2.2349 - mae: 1.1453 - mse: 2.2349 - val_loss: 2.8046 - val_mae: 1.2636 - val_mse: 2.8046\n",
            "Epoch 47/50\n",
            "\u001b[1m518/519\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 2.1939 - mae: 1.1329 - mse: 2.1939\n",
            "Epoch 47: val_loss did not improve from 2.71576\n",
            "\u001b[1m519/519\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - loss: 2.1941 - mae: 1.1330 - mse: 2.1941 - val_loss: 2.8532 - val_mae: 1.2812 - val_mse: 2.8532\n",
            "Epoch 48/50\n",
            "\u001b[1m514/519\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 2.1363 - mae: 1.1223 - mse: 2.1363\n",
            "Epoch 48: val_loss did not improve from 2.71576\n",
            "\u001b[1m519/519\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 2.1376 - mae: 1.1226 - mse: 2.1376 - val_loss: 2.8072 - val_mae: 1.2692 - val_mse: 2.8072\n",
            "Epoch 49/50\n",
            "\u001b[1m506/519\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 2.2357 - mae: 1.1444 - mse: 2.2357\n",
            "Epoch 49: val_loss did not improve from 2.71576\n",
            "\u001b[1m519/519\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 2.2352 - mae: 1.1444 - mse: 2.2352 - val_loss: 2.7674 - val_mae: 1.2513 - val_mse: 2.7674\n",
            "Epoch 50/50\n",
            "\u001b[1m491/519\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 2.1347 - mae: 1.1229 - mse: 2.1347\n",
            "Epoch 50: val_loss did not improve from 2.71576\n",
            "\u001b[1m519/519\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 2.1386 - mae: 1.1238 - mse: 2.1386 - val_loss: 2.7783 - val_mae: 1.2573 - val_mse: 2.7783\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Replace NaNs with 0 or the mean of the column\n",
        "X_test = np.nan_to_num(X_test, nan=0.0)\n",
        "y_test = np.nan_to_num(y_test, nan=0.0)\n"
      ],
      "metadata": {
        "id": "PkOTfkqVQU7m"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluate the best saved model\n",
        "from tensorflow.keras.models import load_model\n",
        "\n",
        "best_model = load_model('best_model.keras')\n",
        "loss, mae, mse = best_model.evaluate(X_test, y_test)\n",
        "print(f\"Best Model Test Loss: {loss}, Test MAE: {mae}, Test MSE: {mse}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XBW97wMJGQgM",
        "outputId": "3dcfb75c-fbe3-4881-f8cd-90e5231496df"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m82/82\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 2.7515 - mae: 1.2591 - mse: 2.7515\n",
            "Best Model Test Loss: 2.594965696334839, Test MAE: 1.2272478342056274, Test MSE: 2.594965696334839\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "import nltk\n",
        "from textblob import TextBlob\n",
        "from collections import Counter\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from nltk.tokenize import sent_tokenize, word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.sentiment import SentimentIntensityAnalyzer\n",
        "from nltk.tag import pos_tag\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "class SingleEssayAnalyzer:\n",
        "    def __init__(self):\n",
        "        # Initialize required NLTK resources\n",
        "        nltk.download('punkt', quiet=True)\n",
        "        nltk.download('averaged_perceptron_tagger', quiet=True)\n",
        "        nltk.download('stopwords', quiet=True)\n",
        "        nltk.download('vader_lexicon', quiet=True)\n",
        "\n",
        "        # Load models and resources\n",
        "        self.nlp = spacy.load('en_core_web_sm')\n",
        "        self.sia = SentimentIntensityAnalyzer()\n",
        "        self.stop_words = set(stopwords.words('english'))\n",
        "\n",
        "    def analyze_essay(self, essay_text):\n",
        "        \"\"\"Analyze a single essay and return all features\"\"\"\n",
        "        # Convert to string to handle any non-string input\n",
        "        essay_text = str(essay_text)\n",
        "\n",
        "        # Extract all features\n",
        "        features = {\n",
        "            **self.get_basic_stats(essay_text),\n",
        "            **self.get_vocabulary_features(essay_text),\n",
        "            **self.get_grammar_features(essay_text),\n",
        "            **self.get_sentence_complexity(essay_text),\n",
        "            **self.get_coherence_features(essay_text),\n",
        "            **self.get_style_features(essay_text),\n",
        "            **self.get_argumentation_features(essay_text)\n",
        "        }\n",
        "\n",
        "        # Convert to DataFrame for better visualization\n",
        "        features_df = pd.DataFrame([features])\n",
        "        return features_df\n",
        "\n",
        "    def get_basic_stats(self, text):\n",
        "        \"\"\"Get basic statistical features\"\"\"\n",
        "        words = word_tokenize(text)\n",
        "        sentences = sent_tokenize(text)\n",
        "        characters = len(text)\n",
        "\n",
        "        return {\n",
        "            'word_count': len(words),\n",
        "            'sentence_count': len(sentences),\n",
        "            'character_count': characters,\n",
        "            'avg_word_per_sentence': len(words) / max(len(sentences), 1),\n",
        "            'avg_char_per_word': characters / max(len(words), 1),\n",
        "            'paragraph_count': len([p for p in text.split('\\n') if p.strip()])\n",
        "        }\n",
        "\n",
        "    def get_vocabulary_features(self, text):\n",
        "        \"\"\"Get vocabulary-related features\"\"\"\n",
        "        words = word_tokenize(text.lower())\n",
        "        content_words = [w for w in words if w.isalnum() and w not in self.stop_words]\n",
        "\n",
        "        if not content_words:\n",
        "            return {\n",
        "                'unique_words': 0,\n",
        "                'lexical_diversity': 0,\n",
        "                'avg_word_length': 0,\n",
        "                'long_words_ratio': 0,\n",
        "                'stopwords_ratio': 0\n",
        "            }\n",
        "\n",
        "        return {\n",
        "            'unique_words': len(set(content_words)),\n",
        "            'lexical_diversity': len(set(content_words)) / len(content_words),\n",
        "            'avg_word_length': np.mean([len(word) for word in content_words]),\n",
        "            'long_words_ratio': sum(1 for word in content_words if len(word) > 6) / len(content_words),\n",
        "            'stopwords_ratio': len([w for w in words if w in self.stop_words]) / len(words)\n",
        "        }\n",
        "\n",
        "    def get_grammar_features(self, text):\n",
        "        \"\"\"Get grammar-related features\"\"\"\n",
        "        doc = self.nlp(text)\n",
        "        total_tokens = len(doc)\n",
        "\n",
        "        if total_tokens == 0:\n",
        "            return {\n",
        "                'noun_ratio': 0,\n",
        "                'verb_ratio': 0,\n",
        "                'adj_ratio': 0,\n",
        "                'adv_ratio': 0,\n",
        "                'pronoun_ratio': 0,\n",
        "                'conjunction_ratio': 0\n",
        "            }\n",
        "\n",
        "        pos_counts = Counter([token.pos_ for token in doc])\n",
        "\n",
        "        return {\n",
        "            'noun_ratio': pos_counts['NOUN'] / total_tokens,\n",
        "            'verb_ratio': pos_counts['VERB'] / total_tokens,\n",
        "            'adj_ratio': pos_counts['ADJ'] / total_tokens,\n",
        "            'adv_ratio': pos_counts['ADV'] / total_tokens,\n",
        "            'pronoun_ratio': pos_counts['PRON'] / total_tokens,\n",
        "            'conjunction_ratio': pos_counts['CCONJ'] / total_tokens\n",
        "        }\n",
        "\n",
        "    def get_sentence_complexity(self, text):\n",
        "        \"\"\"Get sentence complexity features\"\"\"\n",
        "        doc = self.nlp(text)\n",
        "        sentences = list(doc.sents)\n",
        "\n",
        "        if not sentences:\n",
        "            return {\n",
        "                'avg_sentence_length': 0,\n",
        "                'avg_clauses_per_sentence': 0,\n",
        "                'compound_sentence_ratio': 0,\n",
        "                'complex_sentence_ratio': 0\n",
        "            }\n",
        "\n",
        "        clause_counts = []\n",
        "        compound_sentences = 0\n",
        "        complex_sentences = 0\n",
        "\n",
        "        for sent in sentences:\n",
        "            clause_count = 1 + len([token for token in sent\n",
        "                                  if token.dep_ in ['advcl', 'ccomp', 'xcomp']])\n",
        "            clause_counts.append(clause_count)\n",
        "\n",
        "            if any(token.dep_ == 'conj' for token in sent):\n",
        "                compound_sentences += 1\n",
        "\n",
        "            if clause_count > 1:\n",
        "                complex_sentences += 1\n",
        "\n",
        "        return {\n",
        "            'avg_sentence_length': np.mean([len(sent) for sent in sentences]),\n",
        "            'avg_clauses_per_sentence': np.mean(clause_counts),\n",
        "            'compound_sentence_ratio': compound_sentences / len(sentences),\n",
        "            'complex_sentence_ratio': complex_sentences / len(sentences)\n",
        "        }\n",
        "\n",
        "    def get_coherence_features(self, text):\n",
        "        \"\"\"Get coherence-related features\"\"\"\n",
        "        sentences = sent_tokenize(text)\n",
        "\n",
        "        if len(sentences) < 2:\n",
        "            return {\n",
        "                'avg_similarity': 0,\n",
        "                'similarity_variance': 0,\n",
        "                'transition_words_ratio': 0\n",
        "            }\n",
        "\n",
        "        sent_embeddings = [self.nlp(sent).vector for sent in sentences]\n",
        "        similarities = []\n",
        "        for i in range(len(sent_embeddings)-1):\n",
        "            similarity = np.dot(sent_embeddings[i], sent_embeddings[i+1])\n",
        "            similarities.append(float(similarity))\n",
        "\n",
        "        transition_words = set(['however', 'therefore', 'furthermore', 'moreover',\n",
        "                              'nevertheless', 'thus', 'meanwhile', 'consequently',\n",
        "                              'similarly', 'in contrast', 'additionally'])\n",
        "\n",
        "        words = word_tokenize(text.lower())\n",
        "        transition_count = sum(1 for word in words if word in transition_words)\n",
        "\n",
        "        return {\n",
        "            'avg_similarity': np.mean(similarities),\n",
        "            'similarity_variance': np.var(similarities),\n",
        "            'transition_words_ratio': transition_count / len(words)\n",
        "        }\n",
        "\n",
        "    def get_style_features(self, text):\n",
        "        \"\"\"Get style-related features\"\"\"\n",
        "        sentiment = self.sia.polarity_scores(text)\n",
        "        doc = self.nlp(text)\n",
        "\n",
        "        formal_indicators = len([token for token in doc\n",
        "                               if token.pos_ in ['NOUN', 'ADJ', 'NUM']])\n",
        "        informal_indicators = len([token for token in doc\n",
        "                                 if token.pos_ in ['INTJ', 'PART']])\n",
        "\n",
        "        return {\n",
        "            'sentiment_positive': sentiment['pos'],\n",
        "            'sentiment_negative': sentiment['neg'],\n",
        "            'sentiment_neutral': sentiment['neu'],\n",
        "            'sentiment_compound': sentiment['compound'],\n",
        "            'formality_score': formal_indicators / (informal_indicators + 1)\n",
        "        }\n",
        "\n",
        "    def get_argumentation_features(self, text):\n",
        "        \"\"\"Get argumentation-related features\"\"\"\n",
        "        text_lower = text.lower()\n",
        "        words = word_tokenize(text_lower)\n",
        "        total_words = len(words)\n",
        "\n",
        "        if total_words == 0:\n",
        "            return {\n",
        "                'claims_ratio': 0,\n",
        "                'evidence_ratio': 0,\n",
        "                'counter_argument_ratio': 0\n",
        "            }\n",
        "\n",
        "        argument_markers = {\n",
        "            'claims': ['argue', 'claim', 'believe', 'suggest', 'think', 'conclude'],\n",
        "            'evidence': ['because', 'since', 'therefore', 'consequently', 'research', 'study', 'evidence'],\n",
        "            'counter_arguments': ['however', 'although', 'nevertheless', 'despite', 'contrary', 'whereas']\n",
        "        }\n",
        "\n",
        "        counts = {\n",
        "            f'{key}_ratio': sum(1 for marker in markers\n",
        "                               if marker in text_lower) / total_words\n",
        "            for key, markers in argument_markers.items()\n",
        "        }\n",
        "\n",
        "        return counts\n",
        "\n",
        "def analyze_single_essay(essay_text):\n",
        "    \"\"\"Function to analyze a single essay\"\"\"\n",
        "    analyzer = SingleEssayAnalyzer()\n",
        "    features = analyzer.analyze_essay(essay_text)\n",
        "    return features\n",
        "\n"
      ],
      "metadata": {
        "id": "cL782QawGmvI"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('punkt_tab')\n",
        "\n",
        "# Predict a final score for a new essay\n",
        "def predict_final_score(essay_text, model_path):\n",
        "    # Assume `extract_features` is a function that computes all features for a given essay\n",
        "    # Load the pre-trained model and scaler\n",
        "    model = load_model(model_path)\n",
        "    analyzer = SingleEssayAnalyzer()\n",
        "    essay_features = analyzer.analyze_essay(essay_text).values\n",
        "    print(essay_features)\n",
        "    essay_features_scaled = scaler.transform([essay_features[0]])\n",
        "    return best_model.predict(essay_features_scaled)[0][0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W-7UjqMEOXC0",
        "outputId": "bf6b76de-8af7-4cfb-ec34-b0cde8006302"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Package punkt_tab is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Example usage\n",
        "if __name__ == \"__main__\":\n",
        "    # Example essay\n",
        "    sample_essay = \"\"\"\n",
        "    Artificial intelligence (AI) is becoming a part of education. It personalizes learning by making content fit students’ needs and speeds up tasks like grading. This means teachers don’t have to spend as much time doing boring things, so they can focus on teaching. AI also helps students with disabilities by providing tools like text-to-speech or translation apps. However, there are some issues. Privacy is a concern because AI collects data about students. Also, not everyone can afford AI tools, which might make things unfair. Some people think AI could reduce human interaction, which is important in learning. Overall, AI is useful in education but comes with some problems. It can help a lot if used carefully.\n",
        "    \"\"\"\n",
        "    # Specify paths to the model and scaler\n",
        "    model_file = \"/content/best_model.keras\"  # Replace with your actual model path\n",
        "\n",
        "\n",
        "    # Predict score\n",
        "    score = predict_final_score(sample_essay, model_file)\n",
        "\n",
        "    # Display results\n",
        "    print(\"\\nEssay Analysis Results:\")\n",
        "    print(score)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eiY1JhEJxuVd",
        "outputId": "98045705-365e-43f3-b126-7c5e99b94297"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[1.37000000e+02 1.00000000e+01 7.27000000e+02 1.37000000e+01\n",
            "  5.30656934e+00 1.00000000e+00 5.90000000e+01 8.19444444e-01\n",
            "  6.11111111e+00 3.88888889e-01 3.21167883e-01 2.11267606e-01\n",
            "  1.61971831e-01 4.92957746e-02 4.22535211e-02 5.63380282e-02\n",
            "  2.11267606e-02 1.42000000e+01 1.90000000e+00 3.00000000e-01\n",
            "  6.00000000e-01 2.96075467e+00 1.15026277e+00 7.29927007e-03\n",
            "  1.41000000e-01 5.80000000e-02 8.02000000e-01 7.57900000e-01\n",
            "  7.40000000e+00 7.29927007e-03 7.29927007e-03 7.29927007e-03]]\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step\n",
            "\n",
            "Essay Analysis Results:\n",
            "7.454677\n"
          ]
        }
      ]
    }
  ]
}